{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGK/U5jkJEuXq3HTWciO1F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/LangGraph/blob/main/CRAG_LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxE7kfa4VNaN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**LangGraph CRAG**\n",
        "\n",
        "https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag.ipynb?ref=blog.langchain.dev"
      ],
      "metadata": {
        "id": "-9xtjAbVVRX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain langgraph tavily-python --q"
      ],
      "metadata": {
        "id": "qAxliTo9VXrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# APIの設定\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Deep_learning/api.txt\") as file:\n",
        "    #text = file.read()\n",
        "    i=1\n",
        "    key = []\n",
        "    while True:\n",
        "        include_break_line = file.readline() #改行が含まれた行\n",
        "        line = include_break_line.rstrip() #改行を取り除く\n",
        "        if line: #keyの読み込み\n",
        "            #print(f'{i}行目：{line}')\n",
        "            key.append(line)\n",
        "            i += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "# APIキーの準備\n",
        "# #ngrok_aceess_token = key[5]\n",
        "#openai_api_key = key[3]\n",
        "# deepl_auth_key = key[1]\n",
        "# serp_api_key = key[7]\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = key[3]\n",
        "os.environ[\"SERPAPI_API_KEY\"] = key[7]\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = key[9]\n",
        "os.environ[\"GOOGLE_API_KEY\"] = key[11]\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = key[21]\n",
        "os.environ[\"TAVILY_API_KEY\"] = key[25]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5xbDGu4VXt6",
        "outputId": "382cb041-a7a7-47c5-80dc-ed1f6f810484"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Index**"
      ],
      "metadata": {
        "id": "aYmPjlgSWkmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なライブラリをインポート\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# 処理対象のURLを定義\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "\n",
        "# 各URLからコンテンツをロード\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "# ロードした文書を1つのリストにフラット化\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "# テキスト分割器を設定\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=250, chunk_overlap=0\n",
        ")\n",
        "# 文書を小さなチャンクに分割\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "# ベクトルデータベースを作成し、分割した文書を追加\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding=OpenAIEmbeddings(),\n",
        ")\n",
        "# 検索機能（retriever）を設定\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "rPlgFwIHXKR7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 検索結果評価器\n",
        "\n",
        "# 必要なライブラリをインポート\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# データモデルの定義\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"検索された文書の関連性チェックのためのバイナリスコア\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"文書が質問に関連しているかどうか、'yes' または 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# 関数呼び出し機能付きLLMの設定\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# プロンプトの設定\n",
        "system = \"\"\"あなたは、検索された文書がユーザーの質問に関連しているかを評価する採点者です。\\n\n",
        "    文書に質問に関連するキーワードや意味的な内容が含まれている場合、それを関連あるものとして評価してください。\\n\n",
        "    文書が質問に関連しているかどうかを示すために、'yes' または 'no' のバイナリスコアを与えてください。\"\"\"\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"検索された文書: \\n\\n {document} \\n\\n ユーザーの質問: {question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 評価器の作成\n",
        "retrieval_grader = grade_prompt | structured_llm_grader\n",
        "\n",
        "# テスト用の質問と文書の準備\n",
        "question = \"agent memory\"\n",
        "docs = retriever.get_relevant_documents(question)\n",
        "doc_txt = docs[1].page_content\n",
        "\n",
        "# 評価の実行と結果の表示\n",
        "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO6Vv_NjXd18",
        "outputId": "628caebc-fcff-4f68-f1c9-cf8e44f35f0c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "binary_score='yes'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 生成（日本語版）\n",
        "\n",
        "# 必要なライブラリをインポート\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# 日本語用のRAGプロンプトを定義\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"あなたは親切で優秀な日本語のアシスタントです。与えられたコンテキストを使用して、ユーザーの質問に日本語で回答してください。\"),\n",
        "    (\"human\", \"コンテキスト：\\n{context}\\n\\n質問：{question}\\n\\n回答：\")\n",
        "])\n",
        "\n",
        "# LLMの設定\n",
        "# OpenAIのGPT-3.5-turboモデルを使用し、temperature=0で決定論的な出力を得る\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# 後処理関数の定義\n",
        "# 複数の文書を1つの文字列に結合する\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# RAGチェーンの構築\n",
        "# プロンプト、LLM、文字列出力パーサーを組み合わせてチェーンを作成\n",
        "rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# チェーンの実行\n",
        "# コンテキスト（検索された文書）と質問を入力として与え、生成を実行\n",
        "context = format_docs(docs)\n",
        "question_ja = \"エージェントのメモリについて教えてください\"  # 日本語の質問例\n",
        "generation = rag_chain.invoke({\"context\": context, \"question\": question_ja})\n",
        "\n",
        "# 生成結果の表示\n",
        "print(generation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdHOP_7vXlVa",
        "outputId": "da3ef83b-9d48-43e2-fcd0-e1b6fc566227"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "エージェントのメモリは、長期記憶モジュール（外部データベース）であり、自然言語でエージェントの経験の包括的なリストを記録します。このメモリは、過去の経験に基づいてエージェントが行動し、他のエージェントとの相互作用を可能にするためにLLM（大規模言語モデル）と組み合わされています。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 質問リライター（日本語版）\n",
        "\n",
        "# 必要なライブラリをインポート\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# LLMの設定\n",
        "# OpenAIのGPT-3.5-turboモデルを使用し、temperature=0で決定論的な出力を得る\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "\n",
        "# プロンプトの設定\n",
        "system = \"\"\"あなたは質問リライターです。入力された質問をウェブ検索に最適化されたより良いバージョンに変換します。\n",
        "入力を見て、その背後にある意味的な意図や意味について推論してください。\"\"\"\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"以下が元の質問です：\\n\\n {question} \\n より良い質問を作成してください。\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 質問リライターチェーンの構築\n",
        "# プロンプト、LLM、文字列出力パーサーを組み合わせてチェーンを作成\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
        "\n",
        "# テスト実行\n",
        "# サンプルの質問を用意し、リライターを実行\n",
        "sample_question = \"AIエージェントの記憶について教えて\"\n",
        "improved_question = question_rewriter.invoke({\"question\": sample_question})\n",
        "\n",
        "# 結果の表示\n",
        "print(f\"元の質問: {sample_question}\")\n",
        "print(f\"改善された質問: {improved_question}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRgDzaVVYLeq",
        "outputId": "7d94007a-7183-4d1e-a119-a54f1907dc25"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "元の質問: AIエージェントの記憶について教えて\n",
            "改善された質問: AIエージェントが情報をどのように記憶し、それを活用するのかについて詳しく教えてください。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Web Search Tool**"
      ],
      "metadata": {
        "id": "gfqAB3AzY0eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Search\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "web_search_tool = TavilySearchResults(k=3)"
      ],
      "metadata": {
        "id": "mlxuBDVdY4pb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Graph**"
      ],
      "metadata": {
        "id": "UGS1qHMUY61Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Graph State**"
      ],
      "metadata": {
        "id": "spKIaeD1Y-sZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    グラフの状態を表現するクラス。\n",
        "\n",
        "    属性:\n",
        "        question: 質問\n",
        "        generation: LLMによる生成結果\n",
        "        web_search: Web検索を追加するかどうか\n",
        "        documents: 文書のリスト\n",
        "    \"\"\"\n",
        "\n",
        "    question: str  # 質問（文字列）\n",
        "    generation: str  # LLMによる生成結果（文字列）\n",
        "    web_search: str  # Web検索を追加するかどうか（'Yes'または'No'を想定）\n",
        "    documents: List[str]  # 文書のリスト（文字列のリスト）"
      ],
      "metadata": {
        "id": "e9uloTuwZCGI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    文書を検索する\n",
        "\n",
        "    引数:\n",
        "        state (dict): 現在のグラフの状態\n",
        "\n",
        "    戻り値:\n",
        "        state (dict): 検索された文書を含む新しいキー 'documents' が追加された状態\n",
        "    \"\"\"\n",
        "    print(\"---文書検索---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # 検索の実行\n",
        "    documents = retriever.get_relevant_documents(question)\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    回答を生成する\n",
        "\n",
        "    引数:\n",
        "        state (dict): 現在のグラフの状態\n",
        "\n",
        "    戻り値:\n",
        "        state (dict): LLMによる生成結果を含む新しいキー 'generation' が追加された状態\n",
        "    \"\"\"\n",
        "    print(\"---回答生成---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # RAG生成\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "\n",
        "def grade_documents(state):\n",
        "    \"\"\"\n",
        "    検索された文書が質問に関連しているかどうかを判断する\n",
        "\n",
        "    引数:\n",
        "        state (dict): 現在のグラフの状態\n",
        "\n",
        "    戻り値:\n",
        "        state (dict): 関連性のある文書のみでフィルタリングされた 'documents' キーを含む状態\n",
        "    \"\"\"\n",
        "    print(\"---文書の関連性チェック---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # 各文書のスコアリング\n",
        "    filtered_docs = []\n",
        "    web_search = \"No\"\n",
        "    for d in documents:\n",
        "        score = retrieval_grader.invoke(\n",
        "            {\"question\": question, \"document\": d.page_content}\n",
        "        )\n",
        "        grade = score.binary_score\n",
        "        if grade == \"yes\":\n",
        "            print(\"---評価: 文書は関連あり---\")\n",
        "            filtered_docs.append(d)\n",
        "        else:\n",
        "            print(\"---評価: 文書は関連なし---\")\n",
        "            web_search = \"Yes\"\n",
        "            continue\n",
        "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
        "\n",
        "def transform_query(state):\n",
        "    \"\"\"\n",
        "    より良い質問を生成するためにクエリを変換する\n",
        "\n",
        "    引数:\n",
        "        state (dict): 現在のグラフの状態\n",
        "\n",
        "    戻り値:\n",
        "        state (dict): 言い換えられた質問で更新された 'question' キーを含む状態\n",
        "    \"\"\"\n",
        "    print(\"---クエリ変換---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # 質問の書き直し\n",
        "    better_question = question_rewriter.invoke({\"question\": question})\n",
        "    return {\"documents\": documents, \"question\": better_question}\n",
        "\n",
        "def web_search(state):\n",
        "    \"\"\"\n",
        "    言い換えられた質問に基づいてWeb検索を実行する\n",
        "\n",
        "    引数:\n",
        "        state (dict): 現在のグラフの状態\n",
        "\n",
        "    戻り値:\n",
        "        state (dict): Web検索結果が追加された 'documents' キーを含む状態\n",
        "    \"\"\"\n",
        "    print(\"---Web検索---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Web検索の実行\n",
        "    docs = web_search_tool.invoke({\"query\": question})\n",
        "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
        "    web_results = Document(page_content=web_results)\n",
        "    documents.append(web_results)\n",
        "\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "### エッジ\n",
        "\n",
        "def decide_to_generate(state):\n",
        "    \"\"\"\n",
        "    回答を生成するか、質問を再生成するかを決定する\n",
        "\n",
        "    引数:\n",
        "        state (dict): 現在のグラフの状態\n",
        "\n",
        "    戻り値:\n",
        "        str: 次に呼び出すノードの二者択一の決定\n",
        "    \"\"\"\n",
        "    print(\"---評価済み文書の判定---\")\n",
        "    state[\"question\"]\n",
        "    web_search = state[\"web_search\"]\n",
        "    state[\"documents\"]\n",
        "\n",
        "    if web_search == \"Yes\":\n",
        "        # すべての文書が関連性チェックでフィルタリングされた\n",
        "        # 新しいクエリを再生成する\n",
        "        print(\n",
        "            \"---決定: すべての文書が質問に関連していないため、クエリを変換---\"\n",
        "        )\n",
        "        return \"transform_query\"\n",
        "    else:\n",
        "        # 関連する文書があるため、回答を生成\n",
        "        print(\"---決定: 回答生成---\")\n",
        "        return \"generate\""
      ],
      "metadata": {
        "id": "CeXkCco7ZleJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Build Graph**"
      ],
      "metadata": {
        "id": "enbPWWe4ZpBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END, StateGraph, START\n",
        "\n",
        "# GraphStateを使用してStateGraphを初期化\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# ノードの定義\n",
        "workflow.add_node(\"retrieve\", retrieve)  # 文書検索\n",
        "workflow.add_node(\"grade_documents\", grade_documents)  # 文書評価\n",
        "workflow.add_node(\"generate\", generate)  # 回答生成\n",
        "workflow.add_node(\"transform_query\", transform_query)  # クエリ変換\n",
        "workflow.add_node(\"web_search_node\", web_search)  # Web検索\n",
        "\n",
        "# グラフの構築\n",
        "workflow.add_edge(START, \"retrieve\")  # 開始から文書検索へ\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")  # 文書検索から文書評価へ\n",
        "\n",
        "# 条件付きエッジの追加：文書評価の結果に基づいて次のステップを決定\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"transform_query\": \"transform_query\",  # クエリ変換が必要な場合\n",
        "        \"generate\": \"generate\",  # 回答生成が可能な場合\n",
        "    },\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"transform_query\", \"web_search_node\")  # クエリ変換からWeb検索へ\n",
        "workflow.add_edge(\"web_search_node\", \"generate\")  # Web検索から回答生成へ\n",
        "workflow.add_edge(\"generate\", END)  # 回答生成から終了へ\n",
        "\n",
        "# グラフのコンパイル\n",
        "app = workflow.compile()\n",
        "\n",
        "# 使用例：\n",
        "# inputs = {\"question\": \"AIの倫理について教えてください\"}\n",
        "# for output in app.stream(inputs):\n",
        "#     for key, value in output.items():\n",
        "#         print(f\"ノード '{key}'の出力:\")\n",
        "#         print(value)\n",
        "#     print(\"\\n---\\n\")"
      ],
      "metadata": {
        "id": "VfY2KAeXZu2B"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# 実行\n",
        "inputs = {\"question\": \"エージェントのメモリの種類は何ですか？\"}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # ノード名の表示\n",
        "        pprint(f\"ノード '{key}':\")\n",
        "        # オプション: 各ノードでの完全な状態を表示\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# 最終的な生成結果の表示\n",
        "pprint(value[\"generation\"])\n",
        "\n",
        "# 注: このコードを実行すると、ワークフローの各ステップごとに出力が表示されます。\n",
        "# 最後に生成された回答が表示されます。"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N81dckRkZ_M4",
        "outputId": "ae3813d4-e4bb-4607-da68-f56ad0db7503"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---文書検索---\n",
            "\"ノード 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---文書の関連性チェック---\n",
            "---評価: 文書は関連あり---\n",
            "---評価: 文書は関連あり---\n",
            "---評価: 文書は関連あり---\n",
            "---評価: 文書は関連あり---\n",
            "---評価済み文書の判定---\n",
            "---決定: 回答生成---\n",
            "\"ノード 'grade_documents':\"\n",
            "'\\n---\\n'\n",
            "---回答生成---\n",
            "\"ノード 'generate':\"\n",
            "'\\n---\\n'\n",
            "('エージェントのメモリの種類には、次のようなものがあります：\\n'\n",
            " '- 感覚記憶（Sensory Memory）: '\n",
            " '視覚的、聴覚的な情報などの印象を保持し、元の刺激が終了した後も情報を保持する能力を提供します。通常、数秒間しか続かないことが一般的です。視覚的なものをアイコニックメモリ（iconic '\n",
            " 'memory）、聴覚的なものをエコーイックメモリ（echoic memory）、触覚的なものをハプティックメモリ（haptic '\n",
            " 'memory）というサブカテゴリに分類されます。')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# 実行\n",
        "inputs = {\"question\": \"AlphaCodium論文はどのように機能しますか？\"}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # ノード名の表示\n",
        "        pprint(f\"ノード '{key}':\")\n",
        "        # オプション: 各ノードでの完全な状態を表示\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# 最終的な生成結果の表示\n",
        "pprint(value[\"generation\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhfsxbLjaOXv",
        "outputId": "75127a5b-96d2-45ba-9f95-bb201b3a2a26"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---文書検索---\n",
            "\"ノード 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---文書の関連性チェック---\n",
            "---評価: 文書は関連なし---\n",
            "---評価: 文書は関連なし---\n",
            "---評価: 文書は関連なし---\n",
            "---評価: 文書は関連なし---\n",
            "---評価済み文書の判定---\n",
            "---決定: すべての文書が質問に関連していないため、クエリを変換---\n",
            "\"ノード 'grade_documents':\"\n",
            "'\\n---\\n'\n",
            "---クエリ変換---\n",
            "\"ノード 'transform_query':\"\n",
            "'\\n---\\n'\n",
            "---Web検索---\n",
            "\"ノード 'web_search_node':\"\n",
            "'\\n---\\n'\n",
            "---回答生成---\n",
            "\"ノード 'generate':\"\n",
            "'\\n---\\n'\n",
            "('AlphaCodiumは、大規模言語モデル（LLM）を使用してコード生成性能を向上させるための新しいコード生成プロセスです。主な特徴や機能は以下の通りです：\\n'\n",
            " '\\n'\n",
            " '1. GPT-4のコード生成精度を2倍以上向上させることができる。\\n'\n",
            " '2. 生成されたコードを繰り返し実行し、入出力テストに対して修正する反復プロセスを中心にしている。\\n'\n",
            " '3. AlphaCodiumは、競技プログラミングタスクに特化しており、多くの可能な解決策を生成し、その中から最適なものを選ぶ手法を採用している。\\n'\n",
            " '4. 計算コストが比較的少なく、実用的な性能を持っている。\\n'\n",
            " '5. '\n",
            " 'AlphaCodiumは、AlphaCodeと比較してより優れており、はるかに少ない計算資源を使用しながらより良い結果を出していることが示されている。\\n'\n",
            " '\\n'\n",
            " 'これらの特徴により、AlphaCodiumはコード生成の精度と効率を向上させるための有望な手法として注目されています。')\n"
          ]
        }
      ]
    }
  ]
}